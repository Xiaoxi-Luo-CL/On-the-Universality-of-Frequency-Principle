{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "import argparse\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import re\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Default configuration parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description='PyTorch Training for Frequency Principle')\n",
    "\n",
    "\n",
    "parser.add_argument('--lr', default=1e-3, type=float, help='learning rate')\n",
    "parser.add_argument('--optimizer', default='nesterov',\n",
    "                    help='optimizer: sgd | adam | nesterov')\n",
    "parser.add_argument('--epochs', default=100, type=int,\n",
    "                    metavar='N', help='number of total epochs to run')\n",
    "parser.add_argument('--training_batch_size',   default=50, type=int,\n",
    "                    help='the batch size for model (default: 1000)')\n",
    "parser.add_argument('--training_size',   default=5000, type=int,\n",
    "                    help='the training size for model (default: 1000)')\n",
    "parser.add_argument('--test_batch_size',   default=50, type=int,\n",
    "                    help='the test size for model (default: 10000)')\n",
    "parser.add_argument('--act_func_name', default='ReLU',\n",
    "                    help='activation function')\n",
    "parser.add_argument('--in_channel',   default=3, type=int,\n",
    "                    help='the input channel for model (default: 3)')\n",
    "parser.add_argument('--num_classes',   default=10, type=int,\n",
    "                    help='the output dimension for model (default: 10)')\n",
    "parser.add_argument('--device',   default='cuda', type=str,\n",
    "                    help='device used to train (cpu or cuda)')\n",
    "parser.add_argument('--plot_epoch',   default=1, type=int,\n",
    "                    help='step size of plotting interval (default: 1000)')\n",
    "parser.add_argument('--ini_path', type=str,\n",
    "                    default='')\n",
    "parser.add_argument('--start_filter',   default=2, type=float,\n",
    "                    help='the start point of the filter (default: 2)')\n",
    "parser.add_argument('--end_filter',   default=100, type=float,\n",
    "                    help='the end point of the filter (default: 100)')\n",
    "parser.add_argument('--num_filter',   default=20, type=int,\n",
    "                    help='the point number of the filter (default: 20)')\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   The storage path of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save path: 20251013225708452580\n"
     ]
    }
   ],
   "source": [
    "def mkdirs(fn):\n",
    "    \"\"\"\n",
    "    Create directories if they don't exist.\n",
    "\n",
    "    Args:\n",
    "    fn: The directory path to create.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.isdir(fn):\n",
    "        os.makedirs(fn)\n",
    "\n",
    "\n",
    "def create_save_dir(path_ini):\n",
    "    \"\"\"\n",
    "    Create a new directory with the current date and time as its name and return the path of the new directory.\n",
    "\n",
    "    Args:\n",
    "    path_ini: The initial path to create the new directory.\n",
    "\n",
    "    Return:\n",
    "    The path of the new directory.\n",
    "    \"\"\"\n",
    "    subFolderName = re.sub(r'[^0-9]', '', str(datetime.datetime.now()))\n",
    "    path = os.path.join(path_ini, subFolderName)\n",
    "    mkdirs(path)\n",
    "    mkdirs(os.path.join(path, 'output'))\n",
    "    return path\n",
    "\n",
    "\n",
    "args.path = create_save_dir(args.ini_path)\n",
    "print('save path: %s' % (args.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of dataloader.\n",
    "\n",
    "A dataloader is a utility in PyTorch that helps to load and preprocess data for machine learning models. It is used to efficiently load large datasets and feed them to the model in batches during training or inference. \n",
    "\n",
    "The `load_data` function defines a dataloader for the CIFAR10 dataset. The function loads the CIFAR10 dataset using the `datasets.CIFAR10` class and applies the `transform` object to the data. It creates a `Subset` object from the training dataset, which selects a subset of the data based on the `training_size` argument. \n",
    "\n",
    "The `DataLoader` class is then used to create dataloaders for the training and test datasets. The `train_loader` loads the training data in batches of size `training_batch_size`, while the `test_loader` loads the test data in batches of size `test_batch_size`. The `num_workers` argument specifies the number of subprocesses to use for data loading, while the `shuffle` argument specifies whether to shuffle the data before each epoch. \n",
    "\n",
    "\n",
    "This will return two dataloaders: `train_loader` and `test_loader`, which can be used to iterate over the training and test data in batches during training or inference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(training_size, training_batch_size, test_batch_size):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=\"./\",\n",
    "                                    train=True,\n",
    "                                    transform=transform,\n",
    "                                    download=True\n",
    "                                    )\n",
    "\n",
    "    test_dataset = datasets.CIFAR10(root=\"./\",\n",
    "                                    train=False,\n",
    "                                    transform=transform,\n",
    "                                    download=True)\n",
    "\n",
    "    train_dataset=Subset(train_dataset,range(training_size))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=training_batch_size, num_workers=2, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                                 batch_size=test_batch_size,\n",
    "                                 shuffle=False, num_workers=2,drop_last=True, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_data(\n",
    "    args.training_size, args.training_batch_size, args.test_batch_size)\n",
    "\n",
    "train_loader, test_loader=list(train_loader), list(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a 1D datalodaer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DealDataset(Dataset):\n",
    "    def __init__(self, train_X, train_y):\n",
    "        self.x_data = train_X\n",
    "        self.y_data = train_y\n",
    "        self.len = train_X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "class get_1D_data:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def get_target_func(self, x):\n",
    "        return torch.sin(x)+2*torch.sin(3*x)+3*torch.sin(5*x)\n",
    "    \n",
    "    def get_inputs(self):\n",
    "        args = self.args\n",
    "        bound=1\n",
    "\n",
    "        for i in range(2):\n",
    "            if isinstance(args.data_boundary[i], str):\n",
    "                args.data_boundary[i]=eval(args.data_boundary[i])\n",
    "\n",
    "        if args.input_dim == 1:\n",
    "            test_inputs = torch.reshape(torch.linspace(\n",
    "                bound*args.data_boundary[0], bound*args.data_boundary[1], args.test_size), [-1, 1])\n",
    "            train_inputs = torch.reshape(torch.linspace(\n",
    "                args.data_boundary[0], args.data_boundary[1], args.training_size), [-1, 1])\n",
    "        else:\n",
    "            test_inputs = (torch.rand(args.test_size, args.input_dim)\n",
    "                           )*(bound*args.data_boundary[1]-bound*args.data_boundary[0])+bound*args.data_boundary[0]\n",
    "            train_inputs = torch.rand(args.training_size, args.input_dim) *(args.data_boundary[1]-args.data_boundary[0])+args.data_boundary[0]\n",
    "        return test_inputs, train_inputs\n",
    "\n",
    "    def get_data(self):\n",
    "        test_inputs, train_inputs = self.get_inputs()\n",
    "        test_targets, train_targets = self.get_target_func(\n",
    "            test_inputs), self.get_target_func(train_inputs)\n",
    "        train_dataset = DealDataset(\n",
    "            train_inputs, train_targets)\n",
    "        test_dataset = DealDataset(\n",
    "            test_inputs, test_targets)\n",
    "        return train_dataset, test_dataset, test_inputs, train_inputs, test_targets, train_targets\n",
    "\n",
    "\n",
    "def load_data(training_batch_size, test_batch_size, args):\n",
    "    Get_data = get_1D_data(args)\n",
    "    train_dataset, test_dataset, _, _, _, _ = Get_data.get_data()\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                                batch_size=training_batch_size,\n",
    "                                shuffle=False,drop_last=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                                batch_size=test_batch_size,\n",
    "                                shuffle=False,drop_last=True)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network structure:\n",
      "My_CNN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=12544, out_features=400, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=400, out_features=10, bias=True)\n",
      "    (3): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class My_CNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes: int = 10, act_layer: nn.Module = nn.ReLU()):\n",
    "        super(My_CNN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        conv_layers: List[nn.Module] = []\n",
    "\n",
    "        conv_layers += [nn.Conv2d(self.in_channels,\n",
    "                             32, kernel_size=3,stride=1,padding=0), act_layer]\n",
    "        conv_layers += [nn.Conv2d(32, 64, kernel_size=3,stride=1,padding=0), act_layer]\n",
    "        conv_layers += [nn.MaxPool2d(kernel_size=(2, 2))]\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "        mlp_layers: List[nn.Module] = []\n",
    "        mlp_layers += [nn.Linear(14*14*64, 400), act_layer]\n",
    "        mlp_layers += [nn.Linear(400, self.num_classes), nn.Softmax(dim=1)]\n",
    "        self.mlp = nn.Sequential(*mlp_layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self) -> None:\n",
    "        for obj in self.modules():\n",
    "            if isinstance(obj, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.xavier_uniform_(obj.weight.data)\n",
    "\n",
    "\n",
    "def get_act_func(act_func):\n",
    "    \"\"\"\n",
    "    Get activation function.\n",
    "\n",
    "    Args:\n",
    "        act_func (str): activation function name.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: activation function.\n",
    "    \"\"\"\n",
    "    if act_func == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif act_func == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    elif act_func == 'Sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    else:\n",
    "        raise NameError('No such act func!')\n",
    "\n",
    "\n",
    "act_func = get_act_func(args.act_func_name)\n",
    "\n",
    "model = My_CNN(args.in_channel, args.num_classes, act_func).to(args.device)\n",
    "print(\"The network structure:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My_CNN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=12544, out_features=400, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=400, out_features=10, bias=True)\n",
       "    (3): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-step training function.\n",
    "\n",
    "The training data set is denoted as  $S=\\{(x_i,y_i)\\}_{i=1}^n$, where $x_i\\in\\mathbb{R}^d$ and $y_i\\in \\mathbb{R}^{d'}$. For simplicity, we assume an unknown function $y$ satisfying $y(x_i)=y_i$ for $i\\in[n]$. The empirical risk reads as\n",
    "\\begin{equation*}\n",
    "    R_S(\\theta)=\\frac{1}{n}\\sum_{i=1}^n\\ell(f(x_i,\\theta),y(x_i)),\n",
    "\\end{equation*}\n",
    "where the loss function $\\ell(\\cdot,\\cdot)$ is differentiable and the derivative of $\\ell$ with respect to its first argument is denoted by $\\nabla\\ell(y,y^*)$. \n",
    "\n",
    "For a one-step gradient descent, we have, \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\theta_{t+1}=\\theta_t-\\eta\\nabla R_S(\\theta).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, optimizer, loss_fn,  train_loader, args):\n",
    "    \"\"\"\n",
    "    It takes in a model, optimizer, loss function, resultsaver, train_loader, and args, and returns the\n",
    "    average loss and accuracy for the training set\n",
    "\n",
    "    :param model: the model we're training\n",
    "    :param optimizer: the optimizer for training model\n",
    "    :param loss_fn: the loss function\n",
    "    :param train_loader: the training data loader\n",
    "    :param args: a dictionary containing all the parameters for the training process\n",
    "    :return: The average loss and the accuracy\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = args.device\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        batch_size = inputs.size(0)\n",
    "        total += batch_size\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = torch.autograd.Variable(\n",
    "            inputs), torch.autograd.Variable(targets)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(torch.log(outputs), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()*batch_size\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += predicted.eq(targets.data).cpu().sum().item()\n",
    "\n",
    "    acc = 100*correct/total\n",
    "    return train_loss/total, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-step test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, test_loader,  args):\n",
    "    \"\"\"\n",
    "    It takes a model, a test_loader, a loss function, and some arguments, and returns the average loss\n",
    "    and accuracy of the model on the test set.\n",
    "\n",
    "    :param model: the model\n",
    "    :param loss_fn: the loss function\n",
    "    :param test_loader: a DataLoader object\n",
    "    :param args: a dictionary containing all the parameters for the training process\n",
    "    :return: The loss and accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = args.device\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            batch_size = inputs.size(0)\n",
    "            total += batch_size\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs, targets = torch.autograd.Variable(\n",
    "                inputs), torch.autograd.Variable(targets)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(torch.log(outputs), targets)\n",
    "            train_loss += loss.item() * batch_size\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += predicted.eq(targets.data).cpu().sum().item()\n",
    "        acc = 100 * correct / total\n",
    "    return train_loss / total, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to obtain model output on the full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, training_loader,  args):\n",
    "    \"\"\"\n",
    "    It takes a model, a test_loader, a loss function, and some arguments, and returns the average loss\n",
    "    and accuracy of the model on the test set.\n",
    "\n",
    "    :param model: the model\n",
    "    :param training_loader: a DataLoader object\n",
    "    :param args: a dictionary containing all the parameters for the training process\n",
    "    :return: the outputs of the model on the training set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    y_pred =[]\n",
    "\n",
    "    device = args.device\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in training_loader:\n",
    "\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs, targets = torch.autograd.Variable(\n",
    "                inputs), torch.autograd.Variable(targets)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            y_pred.append(outputs)\n",
    "\n",
    "    return torch.cat(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(path, loss_train, x_log=False):\n",
    "    \"\"\"\n",
    "    Plot loss.\n",
    "\n",
    "    Args:\n",
    "        path (str): path.\n",
    "        loss_train (list): list of training loss.\n",
    "        x_log (bool): whether to use log scale for x-axis.\n",
    "\n",
    "    Returns: None.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    y2 = np.asarray(loss_train)\n",
    "    plt.plot(y2, 'k-', label='Train')\n",
    "    plt.xlabel('epoch', fontsize=18)\n",
    "    ax.tick_params(labelsize=18)\n",
    "    plt.yscale('log')\n",
    "    if x_log == False:\n",
    "        fntmp = os.path.join(path, 'loss.jpg')\n",
    "    else:\n",
    "        plt.xscale('log')\n",
    "        fntmp = os.path.join(path, 'loss_log.jpg')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fntmp, dpi=300)\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering\n",
    "For the original dataset $\\left\\{\\left(x_i, y_i\\right)\\right\\}_{i=0}^{n-1}$, such as MNIST or CIFAR10. $x_i$ is an image vector, $y_i$ is a one-hot vector. The low frequency part can be derived by\n",
    "$$\n",
    "y_i^{\\mathrm{low}, \\delta}=\\frac{1}{C_i} \\sum_{j=0}^{n-1} y_j G^\\delta\\left(x_i-x_j\\right),\n",
    "$$\n",
    "where $C_i=\\sum_{j=0}^{n-1} G^\\delta\\left(x_i-x_j\\right)$ is a normalization factor and\n",
    "$$\n",
    "G^\\delta\\left(x_i-x_j\\right)=\\exp \\left(-\\left|x_i-x_j\\right|^2 /(2 \\delta)\\right).\n",
    "$$\n",
    "The high frequency part can be derived by $\\boldsymbol{y}_i^{\\text {high, } \\delta} \\triangleq \\boldsymbol{y}_i-\\boldsymbol{y}_i^{\\text {low }, \\delta}$. We also compute $\\boldsymbol{h}_i^{\\text {low, } \\delta}$ and $\\boldsymbol{h}_i^{\\text {high, } \\delta}$ for each DNN output $\\boldsymbol{h}_i$.\n",
    "Then, we can examine\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& e_{\\text {low }}=\\left(\\frac{\\sum_i\\left|\\boldsymbol{y}_i^{\\text {low }, \\delta}-\\boldsymbol{h}_i^{\\text {low }, \\delta}\\right|^2}{\\sum_i\\left|\\boldsymbol{y}_i^{\\text {low }, \\delta}\\right|^2}\\right)^{\\frac{1}{2}}, \\\\\n",
    "& e_{\\text {high }}=\\left(\\frac{\\sum_i\\left|\\boldsymbol{y}_i^{\\text {high }, \\delta}-\\boldsymbol{h}_i^{\\text {high }, \\delta}\\right|^2}{\\sum_i\\left|y_i^{\\text {high }, \\delta}\\right|^2}\\right)^{\\frac{1}{2}}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_for_training_loader(training_loader):\n",
    "    \"\"\"\n",
    "    Computes the pairwise Euclidean distance between all pairs of flattened images in a training loader.\n",
    "\n",
    "    Args:\n",
    "        training_loader (DataLoader): A DataLoader object containing the training data.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D NumPy array of shape [N, N], where N is the number of images in the training set.\n",
    "    \"\"\"\n",
    "\n",
    "    data_list = [data for data, _ in training_loader]\n",
    "\n",
    "    # Concatenate the list of data into a single tensor\n",
    "    data = torch.cat(data_list)\n",
    "    # Reshape data into [B, C*H*W]\n",
    "    flattened_images = data.view(data.shape[0], -1).numpy()\n",
    "\n",
    "    dist = -2 * np.dot(flattened_images, flattened_images.T) + np.sum(flattened_images**2, axis=1) + \\\n",
    "        np.sum(flattened_images**2, axis=1)[:, np.newaxis]\n",
    "    return dist\n",
    "\n",
    "\n",
    "def normal_kernel(dist, filter_dict):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the normalized Gaussian kernel for each filter in the filter dictionary.\n",
    "\n",
    "    Args:\n",
    "        dist (np.ndarray): A 2D NumPy array of pairwise distances between data points.\n",
    "        filter_dict (list): A list of filter values to use for each kernel.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of 2D NumPy arrays, where each array is a normalized Gaussian kernel for a filter in the filter dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    kernel_dict = []\n",
    "    for filter in filter_dict:\n",
    "        kernel = np.exp(-dist / 2 / filter)\n",
    "        mean = np.sum(kernel, axis=1, keepdims=True)\n",
    "        kernel_dict.append(kernel/mean)\n",
    "    return kernel_dict\n",
    "\n",
    "\n",
    "def gauss_filiter(f_orig, kernel):\n",
    "\n",
    "    \"\"\"\n",
    "    Applies a Gaussian filter to an image output.\n",
    "\n",
    "    Args:\n",
    "        f_orig (np.ndarray): A 2D NumPy array representing the model output.\n",
    "        kernel (np.ndarray): A 2D NumPy array representing the Gaussian kernel.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D NumPy array representing the filtered output.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.matmul(kernel, f_orig)\n",
    "\n",
    "\n",
    "def get_freq_low_high(yy, kernel_dict):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the low and high frequency components of the model output using a set of Gaussian filters.\n",
    "\n",
    "    Args:\n",
    "        yy (np.ndarray): NumPy array representing the model output.\n",
    "        kernel_dict (list): A list of 2D NumPy arrays representing the Gaussian kernels to use for filtering.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of two lists, where the first list contains the low frequency components of\n",
    "                the model output and the second list contains the high frequency components of the model output.\n",
    "    \"\"\"\n",
    "    f_low = []\n",
    "    f_high = []\n",
    "    for filter in range(len(kernel_dict)):\n",
    "        kernel = kernel_dict[filter]\n",
    "        f_new_norm = gauss_filiter(yy, kernel)\n",
    "        f_low.append(f_new_norm)\n",
    "        f_high_tmp = yy - f_new_norm\n",
    "        f_high.append(f_high_tmp)\n",
    "\n",
    "    return f_low, f_high \n",
    "\n",
    "\n",
    "def get_target_freq_distr(train_labels, dist, filter_start, filter_end, filter_num):\n",
    "    \"\"\"\n",
    "    Computes the target frequency distribution of a set of training labels using a set of Gaussian filters.\n",
    "\n",
    "    Args:\n",
    "        train_labels (np.ndarray): NumPy array representing the training labels.\n",
    "        dist (np.ndarray): A 2D NumPy array of pairwise Euclidean distance between all pairs of flattened images in a training loader.\n",
    "        filter_start (float): The starting value of the filter range.\n",
    "        filter_end (float): The ending value of the filter range.\n",
    "        filter_num (int): The number of filters to use in the filter range.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple of four elements, where the first element is a 1D NumPy array of filter values, \n",
    "                the second element is a list of 2D NumPy arrays representing the Gaussian kernels used for filtering, \n",
    "                the third element is a list of 1D NumPy arrays representing the low frequency components of the training labels, \n",
    "                and the fourth element is a list of 1D NumPy arrays representing the high frequency components of the training labels.\n",
    "    \"\"\"\n",
    "\n",
    "    filter_dict = np.linspace(\n",
    "        filter_start, filter_end, num=filter_num)\n",
    "\n",
    "    kernel_dict = normal_kernel(dist, filter_dict)\n",
    "    f_low, f_high = get_freq_low_high(train_labels, kernel_dict)\n",
    "\n",
    "    return filter_dict, kernel_dict, f_low, f_high\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examination. \n",
    "\n",
    "Plot the relative error $e_{\\text {low }}$ and $e_{\\text {high }}$ at each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff_distr(filter, lowdiff, highdiff, args):\n",
    "    \"\"\"\n",
    "    Plots the difference between the low and high frequency components of the predicted labels and the targets.\n",
    "\n",
    "    Args:\n",
    "        filter (float): The filter value used for filtering.\n",
    "        lowdiff (list): A list of relative distances between the low frequency components of the predicted labels and targets.\n",
    "        highdiff (list): A list of relative distances between the high frequency components of the predicted labels and targets.\n",
    "        args (argparse.Namespace): An argparse Namespace object containing the path to save the plot.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.title('freq with filter {:.02f}'.format(filter))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(lowdiff, 'r-', label='low_{:.02f}'.format(filter))\n",
    "    plt.plot(highdiff, 'b-', label='high_{:0.2f}'.format(filter))\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('freq')\n",
    "    plt.subplot(122)\n",
    "    tmp = np.stack([lowdiff, highdiff])\n",
    "    plt.pcolor(tmp, cmap='RdBu', vmin=0.1, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.yticks([0.6, 1.6], ('low freq', 'high freq'), rotation='vertical')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.savefig(args.path + '/hot_{:0.2f}.png'.format(filter))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the high and low frequency distribution of targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dist = compute_distance_for_training_loader(train_loader)\n",
    "\n",
    "target_list = [target for data, target in train_loader]\n",
    "\n",
    "targets = torch.cat(target_list)\n",
    "\n",
    "train_labels = F.one_hot(targets, num_classes=10).detach().cpu().numpy()\n",
    "\n",
    "filter_dict, kernel_dict, f_low, f_high=get_target_freq_distr(train_labels, dist, args.start_filter, args.end_filter, args.num_filter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'new_parser' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n new_parser ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "if args.optimizer == 'sgd':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "elif args.optimizer == 'nesterov':\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), lr=args.lr, momentum=0.9, nesterov=True)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "lowdiff = [[] for _ in range(len(filter_dict))]\n",
    "highdiff = [[] for _ in range(len(filter_dict))]\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "args.loss_training_lst = []\n",
    "\n",
    "for epoch in range(args.epochs+1):\n",
    "    model.train()\n",
    "    loss, acc = train_one_step(\n",
    "        model, optimizer, loss_fn, train_loader, args)\n",
    "    loss_test, acc_test = test(\n",
    "        model, loss_fn, test_loader, args)\n",
    "    y_pred=val(model, train_loader, args)\n",
    "    f_train_low, f_train_high=get_freq_low_high(y_pred.detach().cpu().numpy(), kernel_dict)\n",
    "\n",
    "    for i in range(len(filter_dict)):\n",
    "        lowdiff[i].append(np.linalg.norm(\n",
    "            f_train_low[i] - f_low[i])/np.linalg.norm(f_low[i]))\n",
    "        highdiff[i].append(np.linalg.norm(\n",
    "            f_train_high[i] - f_high[i])/np.linalg.norm(f_high[i]))\n",
    "\n",
    "    args.loss_training_lst.append(loss)\n",
    "\n",
    "    if epoch % args.plot_epoch == 0:\n",
    "          print(\"[%d] loss: %.6f, acc: %.2f, val loss: %.6f, val acc: %.2f, time: %.2f s\" %\n",
    "                (epoch + 1, loss, acc, loss_test, acc_test, (time.time()-t0)))\n",
    "\n",
    "    if (epoch+1) % (args.plot_epoch) == 0:\n",
    "          plot_loss(path=args.path,\n",
    "                    loss_train=args.loss_training_lst, x_log=True)\n",
    "          plot_loss(path=args.path,\n",
    "                    loss_train=args.loss_training_lst, x_log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the frequency difference distribution between the predicted labels and the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'new_parser' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n new_parser ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "for filter_index, filter in enumerate(filter_dict):\n",
    "    lowdiff_ind, highdiff_ind = lowdiff[filter_index], highdiff[filter_index]\n",
    "    plot_diff_distr(filter, lowdiff_ind, highdiff_ind, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
