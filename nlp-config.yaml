# config file for NLP task
dataset:
  ngram: 4
  train_ratio: 0.8
  num_docs: 100000          # number of randomly sampled documents
  ngrams_per_doc: 30        # number of n-grams per document

embedding:
  name: "glove-wiki-gigaword-300"  # word2vec-google-news-300
  dim: 300

model:
  hidden_dims: [1024, 1024, 2048]
  activation: "relu"

training:
  batch_size: 256
  lr: 5e-4
  epochs: 10
  seed: 42
  device: "cuda"
  wandb_log: 1

spectral:
  sample_size: 15000
  scale: "linear" # "linear" or "log"
  interval: 1000
  filter_start: 1
  filter_end: 50
  filter_num: 10

output:
  save_path: "mlp_spectral_wikitext.pt"
