# config file for NLP task
dataset:
  ngram: 4
  train_ratio: 0.8
  num_docs: 100000          # number of randomly sampled documents
  ngrams_per_doc: 10        # number of n-grams per document

embedding:
  name: "glove-wiki-gigaword-300"  # word2vec-google-news-300
  dim: 300
  max_vocab: 50000

model:
  hidden_dims: [1024, 1024, 2048]
  activation: "tanh"

training:
  batch_size: 256
  lr: 1e-3
  epochs: 5
  seed: 42
  device: "cuda"

output:
  save_path: "mlp_spectral_wikitext.pt"
